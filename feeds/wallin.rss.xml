<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Jonas Wallin, mathematical statistics</title><link>/</link><description></description><atom:link href="/feeds/wallin.rss.xml" rel="self"></atom:link><lastBuildDate>Wed, 06 Apr 2016 00:00:00 +0200</lastBuildDate><item><title>Spatially adaptive covariance tapering</title><link>/articles/2016/04/spatially-adaptive-covariance-tapering/</link><description>&lt;p&gt;Using a set of simple compactly supported non-stationary covariance functions, 
combined with a new method for choosing spatially varying taper ranges, we achive an efficent adaptive tapring.
We also explore it for parameter estimation, but here just blocking data is better.&lt;/p&gt;
&lt;p&gt;arxiv:&lt;a href="http://arxiv.org/abs/1506.03670"&gt;Preprint&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bolin</dc:creator><pubDate>Wed, 06 Apr 2016 00:00:00 +0200</pubDate><guid>tag:,2016-04-06:articles/2016/04/spatially-adaptive-covariance-tapering/</guid><category>article</category><category>tapering</category><category>spatial statistics</category></item><item><title>BayesFlow Latent modeling of flow cytometry cell populations</title><link>/articles/2015/05/bayesflow-latent-modeling-of-flow-cytometry-cell-populations/</link><description>&lt;p&gt;Here we formulate a Bayesian hierarchical model, that models the variation across indivuals for 
clow cytometry data. The cell data is follows a Gaussian mixture model, wheras the mean and covariance of each class--representing a certain cell type-- is joint in a latent layer.
Sampling of the posterior distribution is done through a Gibbs sampler, 
where the sampling is parallelised across indivuals.
All (python) code is available at available  &lt;a href="https://github.com/JonasWallin/BayesFlow"&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Extensions:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Lots on the way&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Johnsson</dc:creator><pubDate>Thu, 14 May 2015 00:00:00 +0200</pubDate><guid>tag:,2015-05-14:articles/2015/05/bayesflow-latent-modeling-of-flow-cytometry-cell-populations/</guid><category>article</category><category>MCMC</category><category>GMM</category><category>flow_cytometry</category><category>Hierarchical</category><category>mixture</category></item><item><title>LGFM model</title><link>/articles/2014/03/lgfm-model/</link><description>&lt;p&gt;We introduce the the latent Gaussian random field mixture (LGFM) models.
The model class combines the discrete Markov random fields (MRFs) with latent Gaussian random field models and can be used for simultaneous segmentation, noise reduction, and interpolation of spatial data. The latent model is defined as a mixture of several, possibly multivariate, Gaussian random fields, and which of the fields that is observed at each location is modeled using a discrete random field, such as a discrete MRF.&lt;/p&gt;
&lt;p&gt;As an application we study the noise reduction ability on a magnetic resonance imaging data. In a simulation study (not in the preprint) we also study the ability to detect discontinouties in mean functions of latent Gaussian process. The model preforms much better then the regular latent Gaussian model in recustring the latent fields in both the simulation study and in the noise reduction of the real data. &lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bolin</dc:creator><pubDate>Tue, 25 Mar 2014 00:00:00 +0100</pubDate><guid>tag:,2014-03-25:articles/2014/03/lgfm-model/</guid><category>MRF model</category><category>latent Gaussian process</category><category>estimation</category><category>kriging</category><category>mri imaging</category></item><item><title>Slepian models for LMA</title><link>/articles/2014/03/slepian-models-for-lma/</link><description>&lt;p&gt;The article describes the concept of Slepian noise. A slepian models describes the behavior of a stochastic process (in time) crossing a certain level, $u$. The main contribution of the articles are method to simulate slepian for Laplace moving average models. That is models of the type 
$$
 X(t) = \int f(t-s) d\Lambda(s),
$$
where $\Lambda(s)$ is Laplace moving average. &lt;/p&gt;
&lt;p&gt;We compare the Slepian Laplace model to the regular Gaussian version on data that micis cars driving on roads. The figure displays the upcrossing of the accelaration of a person given that drives on a Laplace moving average road. 
R ana matlab code for the Laplace crossing&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Podg√≥rski</dc:creator><pubDate>Tue, 25 Mar 2014 00:00:00 +0100</pubDate><guid>tag:,2014-03-25:articles/2014/03/slepian-models-for-lma/</guid><category>Laplace moving average</category><category>Slepian</category><category>stochastic process</category><category>upcrossing</category></item><item><title>non-Gaussian Matern Fields</title><link>/articles/2013/07/non-gaussian-matern-fields/</link><description>&lt;p&gt;In this article, we exaim how to preform estimation and predicition on a certain type of continous non Gaussian random fields models.
The article is an application extension of the article &lt;a href="http://arxiv.org/abs/1206.0622"&gt;Bolin 2013&lt;/a&gt; (now published in SJS).
We extend the models in the article, by adding measurement noise and covariates; the estimation
is done through a MCEM algorithm. 
We also test how the models preform on a real data set with precipitation over the US; comparing the model to the
standard Gaussian and transformed Gaussian models. The conlusion for this data is that the models works well
but we need to incorporate covariates into the variance, since the mean and variance of the data is clearly linked.&lt;/p&gt;
&lt;p&gt;R code is available at: &lt;a href="https://bitbucket.org/davidbolin/lang"&gt;Lang&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Extensions:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The next planned extension is to change from MCEM to other stochastic estimation methods; There are two main reasons
    for this, first hopefully we can improve the speed of the algorithm, secondly we hope it allowes for more advanced models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Expanding the model for regular grids; where a more rich noise class can be studied.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bolin</dc:creator><pubDate>Wed, 24 Jul 2013 00:00:00 +0200</pubDate><guid>tag:,2013-07-24:articles/2013/07/non-gaussian-matern-fields/</guid><category>article</category><category>generalized Hyperbolic disribution</category><category>MCEM</category><category>SPDEs</category><category>spatial</category><category>precipitation</category></item><item><title>Convolution of Generalized Hyperbolic</title><link>/articles/2013/06/convolution-of-generalized-hyperbolic/</link><description>&lt;hr /&gt;
&lt;p&gt;In this article, we show that the only sub distributions of &lt;a href="http://en.wikipedia.org/wiki/Generalised_hyperbolic_distribution"&gt;the Generalized hyperbolic distribution&lt;/a&gt; that are closed under convolution are &lt;a href="http://en.wikipedia.org/wiki/Normal-inverse_Gaussian_distribution"&gt;the Normal Inverse Gaussian distribution (NIG)&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Variance-gamma_distribution"&gt;the Generalized Laplace distribution (GAL)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The results are derived through showing that &lt;a href="http://en.wikipedia.org/wiki/Gamma_distribution"&gt;the Gamma distribution&lt;/a&gt; and the &lt;a href="http://en.wikipedia.org/wiki/Inverse_Gaussian_distribution"&gt;Inverse Gaussian distribution&lt;/a&gt; are the only sub distributions of &lt;a href="http://en.wikipedia.org/wiki/Generalized_inverse_Gaussian_distribution"&gt;the Generalized inverse Gaussian distribution&lt;/a&gt; that are closed under distribution.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Podgorski</dc:creator><pubDate>Sat, 15 Jun 2013 00:00:00 +0200</pubDate><guid>tag:,2013-06-15:articles/2013/06/convolution-of-generalized-hyperbolic/</guid><category>generalized Hyperbolic disribution</category><category>normal inverse Gaussian</category><category>Laplace distribution</category><category>convolution invariant</category></item><item><title>LeaveOneOut</title><link>/articles/2013/06/leaveoneout/</link><description>&lt;p&gt;In this article, we study an estimator of the location parameters ($\delta$) when the
density grows as $cx^{-1}$ when $x \rightarrow 0$. The idea is simply to remove the density ,$f(x_i-\delta)$, coming from observation $i=\argmin_j |x_j-\delta|$ from the log likelihood to get the LeaveOneOut&lt;/p&gt;
&lt;p&gt;$$
    L(\delta) = \sum_{j \neq i} \log(f(x_i-\delta)).
$$&lt;/p&gt;
&lt;h4&gt;Extensions:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The optimal rate of convergence it not achived. Is this due to 
the method is lacking, or due to the proof is lacking?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We study this class of densities manly beacuse that the &lt;a href="https://en.wikipedia.org/wiki/Variance-gamma_distribution"&gt;Generalized Laplace distribution&lt;/a&gt; (Variance Gamma) is contained in the class. It remains to show that consisteny for the LeaveOneOut likelihood for all the parameters of the distribution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Podgorski</dc:creator><pubDate>Fri, 14 Jun 2013 00:00:00 +0200</pubDate><guid>tag:,2013-06-14:articles/2013/06/leaveoneout/</guid><category>consistent estimator</category><category>unbounded density</category><category>article</category><category>Laplace distribution</category></item></channel></rss>